---
title: "Paper Visualization"
author: "Emerson Johnston"
lastmodifeddate: "2024-09-01"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Maintenance for R and Python
```{r Reset and Setup}
rm(list = ls()) 
knitr::opts_knit$set(root.dir = '/Users/emerson/Github/usenet_AIDs_discourses_webpage')
```

```{r R Load Libraries, Directories, and Datasets}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(igraph)
library(ggraph)
library(tidytext)
library(topicmodels)
library(visNetwork)
library(RColorBrewer)  

# Load Directories
output_directory <- "/Users/emerson/Github/usenet_AIDs_discourses_webpage"
threads_directory <- "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Threads"
comments_directory <- "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Comments"

# Load the datasets
all_threads <- read.csv(file.path(threads_directory, "aids_related_threads_82TO86.csv"))
all_comments <- read.csv(file.path(comments_directory, "aids_related_comments_82TO86.csv"))
```

```{r R CSV Cleaning}
all_threads <- all_threads %>% drop_na()
all_comments <- all_comments %>% drop_na()
all_comments$Date <- as.Date(all_comments$Date, format = "%Y-%m-%d")
all_threads$Date <- as.Date(all_threads$Date, format = "%Y-%m-%d")
```

```{python Python Load Libraries, Directories, and Datasets}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
import scipy

# Define directories
output_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage/"
threads_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Threads/"
comments_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Comments/"

# Load the datasets with the correct file paths
all_threads = pd.read_csv(os.path.join(threads_directory, "aids_related_threads_82TO86.csv"))
all_comments = pd.read_csv(os.path.join(comments_directory, "aids_related_comments_82TO86.csv"))
```

# Theme Prevelance Hypothesis
```{r Data Preparation}
library(topicmodels)
library(tm)
library(SnowballC)
library(tidytext)
library(dplyr)
library(ggplot2)
library(igraph)
library(ggraph)

# Data preparation: Select relevant text data
comments_corpus <- Corpus(VectorSource(all_comments$Full.Text)) # Replace 'Content' with the actual column name containing text data

# Text preprocessing
comments_corpus <- comments_corpus %>%
  tm_map(content_transformer(tolower)) %>%  # Convert text to lowercase
  tm_map(removePunctuation) %>%              # Remove punctuation
  tm_map(removeNumbers) %>%                  # Remove numbers
  tm_map(removeWords, stopwords("english")) %>%  # Remove common stop words
  tm_map(stemDocument) %>%                   # Perform stemming
  tm_map(stripWhitespace)                    # Strip whitespace

# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(comments_corpus)

# Remove sparse terms to reduce the size of DTM
dtm <- removeSparseTerms(dtm, 0.92) # Adjust sparsity threshold if necessary

# Check for rows with all zero entries and remove them
row_totals <- apply(dtm, 1, sum)
dtm <- dtm[row_totals > 0, ]
```

```{r How Many Topics, eval=FALSE, include=FALSE}
library(ldatuning)

# Sample a smaller subset of the DTM
set.seed(123)
sample_indices <- sample(1:nrow(dtm), 1000)  # Reduce size to speed up
sampled_dtm <- dtm[sample_indices, ]

# Optimize LDA to find the best number of topics
set.seed(123)
result <- FindTopicsNumber(
  sampled_dtm,
  topics = seq(from = 2, to = 10, by = 1),  # Narrow topic range for faster computation
  metrics = c("Griffiths2004", "Deveaud2014"),  # Use fewer metrics
  method = "Gibbs",
  control = list(seed = 123, iter = 500),  # Fewer iterations for quicker results
  mc.cores = 6L,
  verbose = TRUE
)

# Plot results to determine the optimal number of topics
FindTopicsNumber_plot(result)
```

```{r LDA Model}
set.seed(123) # Set seed for reproducibility
lda_model <- LDA(dtm, k = 8, control = list(seed = 123)) # 'k' is the number of topics, adjust as needed
```

```{Top Terms}
# Display the top terms for each topic
top_terms <- terms(lda_model, 10)
print(top_terms)

# Visualize the top terms for each topic (concise visualization)
top_terms_df <- tidy(lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Plotting the top terms for each topic
ggplot(top_terms_df, aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top Terms in Each Topic", x = "Term", y = "Beta") +
  theme_minimal()
```

```{r Co-occurrence Network Analysis}
# Create a co-occurrence matrix
term_matrix <- as.matrix(dtm)
co_occurrence <- crossprod(term_matrix)

# Build graph object
graph <- graph_from_adjacency_matrix(co_occurrence, weighted = TRUE, mode = "undirected", diag = FALSE)
V(graph)$name <- colnames(co_occurrence)

# Simplify graph (remove loops and multiple edges)
graph <- simplify(graph)

# Visualize co-occurrence network
set.seed(123)
ggraph(graph, layout = 'fr') + 
  geom_edge_link(aes(edge_alpha = weight), show.legend = FALSE) + 
  geom_node_point(color = "blue", size = 5) + 
  geom_node_text(aes(label = name), repel = TRUE) + 
  theme_void() +
  labs(title = "Co-occurrence Network of Themes")
```


# Emotional Tone Hypothesis


# Author Impact Hypothesis
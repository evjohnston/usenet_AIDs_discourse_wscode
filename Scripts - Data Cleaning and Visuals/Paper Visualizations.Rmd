---
title: "Paper Visualization"
author: "Emerson Johnston"
lastmodifeddate: "2024-09-01"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Maintenance for R and Python
```{r Reset and Setup}
rm(list = ls()) 
knitr::opts_knit$set(root.dir = '/Users/emerson/Github/usenet_AIDs_discourses_webpage')
```

```{r R Load Libraries, Directories, and Datasets}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(igraph)
library(ggraph)
library(tidytext)
library(topicmodels)
library(visNetwork)
library(RColorBrewer) 
library(sjPlot)
library(ldatuning)
library(tm)
library(SnowballC)

# Load Directories
output_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage"
threads_directory <- "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Threads"
comments_directory <- "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Comments"

# Load the datasets
all_threads <- read.csv(file.path(threads_directory, "aids_related_threads_82TO86.csv"))
all_comments <- read.csv(file.path(comments_directory, "aids_related_comments_82TO86.csv"))
```

```{r R CSV Cleaning}
all_threads <- all_threads %>% drop_na()
all_comments <- all_comments %>% drop_na()
all_comments$Date <- as.Date(all_comments$Date, format = "%Y-%m-%d")
all_threads$Date <- as.Date(all_threads$Date, format = "%Y-%m-%d")
```

```{python Python Load Libraries, Directories, and Datasets, eval=FALSE, include=FALSE}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
import scipy

# Define directories
output_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage/"
threads_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Threads/"
comments_directory = "/Users/emerson/Github/usenet_AIDs_discourses_webpage/CSV Files/Comments/"

# Load the datasets with the correct file paths
all_threads = pd.read_csv(os.path.join(threads_directory, "aids_related_threads_82TO86.csv"))
all_comments = pd.read_csv(os.path.join(comments_directory, "aids_related_comments_82TO86.csv"))
```

```{r Filtering For Relevancy}
# Calculate the minimum and maximum of the 'Thread_Relevancy' column
min_value <- min(all_threads$Thread_Relevancy, na.rm = TRUE)
max_value <- max(all_threads$Thread_Relevancy, na.rm = TRUE)

# Create four equal-sized bins between the min and max values
breaks <- seq(min_value, max_value, length.out = 5) 

# Create a factor for quartile ranges based on these breaks
all_threads$Quartile <- cut(
  all_threads$Thread_Relevancy,
  breaks = breaks,
  labels = c("Q1", "Q2", "Q3", "Q4"),
  include.lowest = TRUE
)

# Plot the bar graph showing the frequency of each quartile
ggplot(all_threads, aes(x = Quartile)) +
  geom_bar(fill = "skyblue", color = "black") +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +  # Add labels above bars
  labs(title = "Frequency of Thread Relevancy Quartiles",
       x = "Quartile",
       y = "Frequency") +
  theme_minimal()

# Define the bin width manually
bin_width <- 0.5

# Plot the histogram with density overlay using all_threads
ggplot(all_threads, aes(x = Thread_Relevancy)) +
  geom_histogram(aes(y = ..count..), binwidth = bin_width, fill = "skyblue", color = "black", alpha = 0.7) + 
  geom_density(aes(y = ..density.. * length(all_threads$Thread_Relevancy) * bin_width), fill = "orange", alpha = 0.4) +  # Scale density to match histogram counts
  labs(title = "Histogram and Density of Thread Relevancy Scores",
       x = "Relevancy",
       y = "Number of Threads") +
  theme_minimal()
```

```{r Filtering Out Quartile 1}
filtered_threads <- all_threads %>%
  filter(Quartile != "Q1")

# Plot the bar graph showing the frequency of each quartile
ggplot(filtered_threads, aes(x = Quartile)) +
  geom_bar(fill = "skyblue", color = "black") +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +  # Add labels above bars
  labs(title = "Frequency of Thread Relevancy Quartiles",
       x = "Quartile",
       y = "Frequency") +
  theme_minimal()

# Define the bin width manually
bin_width <- 0.5

# Plot the histogram with density overlay using all_threads
ggplot(filtered_threads, aes(x = Thread_Relevancy)) +
  geom_histogram(aes(y = ..count..), binwidth = bin_width, fill = "skyblue", color = "black", alpha = 0.7) + 
  geom_density(aes(y = ..density.. * bin_width * nrow(all_threads)), fill = "orange", alpha = 0.4) +  # Scale density to match histogram counts
  labs(title = "Histogram and Density of Thread Relevancy Scores",
       x = "Relevancy",
       y = "Number of Threads") +
  theme_minimal()
```
```{r Get Filtered Comments from Filtered Threads}
# Filter threads with only one comment out of filtered threads 
filtered_threads <- filtered_threads %>%
  filter(Number.of.Messages > 1)

# Filter the comments from 'all_comments' that match the filtered threads
filtered_comments <- all_comments %>%
  filter(Thread.ID %in% filtered_threads$Unique_ThreadID)

# Check the result
head(filtered_comments)
```

```{r Relevancy Filtered Descriptive Statistics}
# Step 1: Group by newsgroup and calculate required statistics, including comment relevancy
keywords_summary_df <- filtered_comments %>%
  group_by(newsgroup) %>%
  summarize(
    Unique_Authors = n_distinct(Author),
    Comments_with_Keywords = sum(Relevancy > 0),
    Conversations_Mentioned_In = n_distinct(Thread.ID),
    Total_Comments_In_Threads = n(),
    Percent_Total_Comments_w_Keyword = (Comments_with_Keywords / Total_Comments_In_Threads) * 100,
    Average_Relevancy_Score = mean(Relevancy, na.rm = TRUE)
  )

# Step 2: Calculate thread relevancy averages for filtered threads
threads_summary_df_grouped <- filtered_threads %>%
  group_by(newsgroup) %>%
  summarize(Average_Thread_Relevancy = mean(Thread_Relevancy, na.rm = TRUE))

# Step 3: Combine comment and thread summaries
keywords_summary_df <- keywords_summary_df %>%
  left_join(threads_summary_df_grouped, by = "newsgroup")

total_unique_authors <- n_distinct(filtered_comments$Author)

# Step 4: Add a totals row
totals <- keywords_summary_df %>%
  summarize(
    newsgroup = "Total",
    Comments_with_Keywords = sum(Comments_with_Keywords),
    Conversations_Mentioned_In = sum(Conversations_Mentioned_In),
    Total_Comments_In_Threads = sum(Total_Comments_In_Threads),
    Unique_Authors = total_unique_authors,
    Percent_Total_Comments_w_Keyword = (Comments_with_Keywords / Total_Comments_In_Threads) * 100,
    Average_Relevancy_Score = mean(Average_Relevancy_Score, na.rm = TRUE),
    Average_Thread_Relevancy = mean(Average_Thread_Relevancy, na.rm = TRUE)
  )

# Step 5: Combine the summary with the totals row
keywords_summary_df <- bind_rows(keywords_summary_df, totals)

# Correctly construct the output file path
output_file_path <- file.path(output_directory, "images and tables", "aids_descriptive_statistics_table_relevancy_filtered.html")

# Ensure the directory exists
dir.create(dirname(output_file_path), recursive = TRUE, showWarnings = FALSE)

# Use the constructed path for saving the file
tab_df(keywords_summary_df, file = output_file_path)
```

# Theme Prevelance Hypothesis
```{r Data Preparation}
# Data preparation: Select relevant text data
comments_corpus <- Corpus(VectorSource(filtered_comments$Full.Text)) # Replace 'Content' with the actual column name containing text data

# Text preprocessing
comments_corpus <- comments_corpus %>%
  tm_map(content_transformer(tolower)) %>%  # Convert text to lowercase
  tm_map(removePunctuation) %>%              # Remove punctuation
  tm_map(removeNumbers) %>%                  # Remove numbers
  tm_map(removeWords, stopwords("english")) %>%  # Remove common stop words
#  tm_map(stemDocument) %>%                   # Perform stemming
  tm_map(stripWhitespace)                    # Strip whitespace

# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(comments_corpus)

# Remove sparse terms to reduce the size of DTM
dtm <- removeSparseTerms(dtm, 0.90) # Adjust sparsity threshold if necessary

# Check for rows with all zero entries and remove them
row_totals <- apply(dtm, 1, sum)
dtm <- dtm[row_totals > 0, ]
```

```{r How Many Topics, eval=FALSE, include=FALSE}
# Optimize LDA to find the best number of topics
set.seed(123)
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 10, by = 1),  # Narrow topic range for faster computation
  metrics = c("Griffiths2004", "Deveaud2014"),  # Use fewer metrics
  method = "Gibbs",
  control = list(seed = 123, iter = 500),  # Fewer iterations for quicker results
  mc.cores = 6L,
  verbose = TRUE
)
```

```{r Plot How Many Topics}
# Plot results to determine the optimal number of topics
FindTopicsNumber_plot(result)
```

```{r LDA Model}
set.seed(123) # Set seed for reproducibility
lda_model <- LDA(dtm, k = 5, control = list(seed = 123)) # 'k' is the number of topics, adjust as needed
```

```{r Top Terms}
# Load required libraries
library(tidyr)
library(dplyr)
library(sjPlot)

# Assuming top_terms_df is your data frame from the LDA analysis

# Reshape the data frame to have each topic's terms and betas side by side
reshaped_df <- top_terms_df %>%
  group_by(topic) %>%
  mutate(term_id = row_number()) %>%
  ungroup() %>%
  pivot_wider(
    names_from = topic,
    values_from = c(term, beta),
    names_glue = "Topic_{topic}_{.value}"
  )

reshaped_df <- reshaped_df %>%
  select(term_id, 
         starts_with("Topic_1_term"), starts_with("Topic_1_beta"),
         starts_with("Topic_2_term"), starts_with("Topic_2_beta"),
         starts_with("Topic_3_term"), starts_with("Topic_3_beta"),
         starts_with("Topic_4_term"), starts_with("Topic_4_beta"),
         starts_with("Topic_5_term"), starts_with("Topic_5_beta"))

# Create a styled table using sjPlot
# tab_df(
#  reshaped_df,
#  title = "Top Terms and Betas for Each Topic",
#  show.rownames = FALSE,
#  digits = 2,
#  file = output_file_path
#  )

library(dplyr)
library(gt)

# Create the gt table
gt_table <- reshaped_df %>%
  gt() %>%
  tab_spanner(
    label = "Topic 1",
    columns = c("Topic_1_term", "Topic_1_beta")
  ) %>%
  tab_spanner(
    label = "Topic 2",
    columns = c("Topic_2_term", "Topic_2_beta")
  ) %>%
  tab_spanner(
    label = "Topic 3",
    columns = c("Topic_3_term", "Topic_3_beta")
  ) %>%
  tab_spanner(
    label = "Topic 4",
    columns = c("Topic_4_term", "Topic_4_beta")
  ) %>%
  tab_spanner(
    label = "Topic 5",
    columns = c("Topic_5_term", "Topic_5_beta")
  ) %>%
  cols_label(
    term_id = "Term No.",
    Topic_1_term = "term", Topic_1_beta = "beta",
    Topic_2_term = "term", Topic_2_beta = "beta",
    Topic_3_term = "term", Topic_3_beta = "beta",
    Topic_4_term = "term", Topic_4_beta = "beta",
    Topic_5_term = "term", Topic_5_beta = "beta"
  )

gt_table

# Save the gt table to an HTML file
output_file_path <- file.path(output_directory, "images and tables", "Top Terms and Betas for Each Topic.html")
gtsave(gt_table, output_file_path)
```

```{r Improved Co-occurrence Network Analysis}
library(ggforce)
library(ggrepel)
library(ggplot2)
library(igraph)

# Step 1: Calculate Term Frequencies and Filter for Top Terms
co_occurrence <- crossprod(as.matrix(dtm)) # Create co-occurrence matrix
# Set a threshold to filter less relevant terms based on frequency
term_frequency <- rowSums(co_occurrence) # Calculate term frequency
threshold <- quantile(term_frequency, 0.75) # Set threshold for top 25% most frequent terms
filtered_terms <- which(term_frequency > threshold)
filtered_co_occurrence <- co_occurrence[filtered_terms, filtered_terms]

# Step 2: Create a Graph Object with Filtered Terms
graph <- graph_from_adjacency_matrix(filtered_co_occurrence, weighted = TRUE, mode = "undirected", diag = FALSE)

# Step 3: Simplify the Graph to Avoid Self-Loops and Redundant Edges
graph <- simplify(graph)

# Step 4: Assign Topics to Terms Based on the LDA Model
terms <- rownames(filtered_co_occurrence) # Extract terms from the filtered co-occurrence matrix
term_topic_probs <- posterior(lda_model)$terms[, terms] # Extract term-topic probabilities for the filtered terms
term_topic_assignment <- apply(term_topic_probs, 2, which.max) # Assign each term to the topic with the highest probability

# Create a data frame mapping terms to topics
term_topic_df <- data.frame(term = terms, topic = factor(term_topic_assignment))

# Increase the repulsion and area in the layout algorithm
set.seed(123)  # For reproducibility
graph_layout <- create_layout(graph, layout = layout_with_fr(graph, niter = 5000, area = vcount(graph)^3.5, repulserad = vcount(graph)^3.5))

# Normalize the layout to ensure it fits within the plot
graph_layout$x <- scale(graph_layout$x, center = TRUE, scale = TRUE) * 4  # Increase spread
graph_layout$y <- scale(graph_layout$y, center = TRUE, scale = TRUE) * 4  # Increase spread

# Create intuitive topic names based on LDA results
topic_names <- c(
  "Medical Testing & Treatment",
  "Personal Experiences & Opinions",
  "Disease Research & Transmission",
  "Sexual Orientation & Behavior",
  "Social Discussion & Impact"
)
term_topic_df$topic_name <- topic_names[term_topic_df$topic]

# Create the improved plot with simplified legend
co_occurrence_plot <- ggplot(graph_layout, aes(x = x, y = y)) +
  geom_edge_link(aes(edge_alpha = weight), edge_width = 0.3, color = "gray70", alpha = 0.3) +
  geom_node_point(aes(size = log1p(term_frequency[filtered_terms]), color = term_topic_df$topic_name), show.legend = TRUE, alpha = 0.8) +
  geom_text_repel(aes(label = name), 
                  max.overlaps = 20,
                  force = 10,
                  point.padding = 0.5,
                  segment.color = "gray50",
                  min.segment.length = 0,
                  box.padding = 0.7,
                  segment.size = 0.2,
                  size = 8) +
  geom_mark_hull(aes(group = term_topic_df$topic_name, fill = term_topic_df$topic_name), 
                 alpha = 0.1, show.legend = FALSE) +
  scale_size(range = c(2, 12), name = "Term Frequency") +
  scale_color_brewer(palette = "Set1", name = "Topic") +
  scale_fill_brewer(palette = "Set1", guide = "none") +  # Remove fill from legend
  coord_cartesian(xlim = c(-8, 8), ylim = c(-8, 8)) +
  theme_void(base_size = 16) +
  theme(
    aspect.ratio = 1,
    legend.position = "right",
    legend.box = "vertical",
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
    plot.margin = margin(40, 40, 40, 40),
    plot.background = element_rect(fill = "white", color = NA),
    legend.text = element_text(size = 10),
    legend.title = element_text(size = 12)
  ) +
  guides(color = guide_legend(override.aes = list(size = 5))) +
  labs(title = "Co-occurrence Network of Key Themes in AIDS Discussion",
       caption = "Node size represents term frequency. Colors indicate topics identified by LDA.")

# Display the plot
print(co_occurrence_plot)

# Save the plot to a PNG file with increased size
ggsave(file.path(output_directory, "images and tables", "co-occurrence_network.png"), 
       plot = co_occurrence_plot, width = 20, height = 16, dpi = 300)  # Increased dimensions
```

# Emotional Tone Hypothesis


# Author Impact Hypothesis